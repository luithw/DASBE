{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import gain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_single_data,\n",
    "    train_multi_data,\n",
    "    test_data,\n",
    "    train_single_label,\n",
    "    train_multi_label,\n",
    "    test_label,\n",
    ") = gain_dataset(\"./tmp_data\", \"shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_single_data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(train_single_data)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axes[1]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(train_single_label)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_multi_data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(train_multi_data)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axes[1]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(train_multi_label).div(3)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_multi_data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(test_data)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axes[1]\n",
    "img = torchvision.utils.make_grid(\n",
    "    torch.as_tensor(test_label).div(3)[:5].unsqueeze(1), nrow=5, pad_value=1\n",
    ")\n",
    "ax.imshow(img.permute(1, 2, 0))\n",
    "# remove frame\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "square = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "triangle = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "shapes = [square, triangle, triangle.flip(dims=(0,)).clone()]\n",
    "class_names = [\"square\", \"triangle\", \"inverted-triangle\"]\n",
    "\n",
    "\n",
    "def generate_shapes_image(width, height, nr_shapes=2):\n",
    "    img = torch.zeros(height, width)\n",
    "    cluster = torch.zeros(nr_shapes, height, width)\n",
    "    target = torch.randint(0, len(shapes), (nr_shapes,))\n",
    "\n",
    "    for i in range(nr_shapes):\n",
    "        shape_idx = target[i]\n",
    "        shape = shapes[shape_idx]\n",
    "        sy, sx = shape.shape\n",
    "        x = torch.randint(0, width - sx + 1, ())\n",
    "        y = torch.randint(0, height - sy + 1, ())\n",
    "        region = (slice(y, y + sy), slice(x, x + sx))\n",
    "        img[region] += shape\n",
    "        cluster[i][region] += shape\n",
    "\n",
    "    img = img.clamp_max_(1)\n",
    "    return img, cluster, target\n",
    "\n",
    "\n",
    "plt.imshow(generate_shapes_image(28, 28, 2)[0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_context_like(tensor):\n",
    "    return torch.randn_like(tensor).abs()\n",
    "\n",
    "\n",
    "def random_mask_like(tensor, p):\n",
    "    return torch.empty_like(tensor).bernoulli_(p)\n",
    "\n",
    "\n",
    "def flatten(intput):\n",
    "    return intput.view(intput.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_any_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Forces the network to predict any one (but only one) of the targets.\n",
    "    It picks the class that is already most similar to the target.\n",
    "\n",
    "    Args:\n",
    "        logits: (N, C)\n",
    "        targets: (N, n) where n is the number of targets per sample\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        candidates = torch.gather(logits, dim=1, index=targets)\n",
    "        indices = torch.argmax(candidates, dim=1, keepdim=True)\n",
    "        most_likely = torch.gather(targets, dim=1, index=indices)\n",
    "        most_likely.squeeze_(dim=1)\n",
    "\n",
    "    return F.cross_entropy(logits, most_likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_cluster_loss(logits, clusters):\n",
    "    with torch.no_grad():\n",
    "        similarities = torch.bmm(\n",
    "            logits.unsqueeze(1), clusters.transpose(-1, -2)\n",
    "        ).squeeze(1)\n",
    "        max_similar = torch.argmax(similarities, dim=1)\n",
    "        targets = clusters[torch.arange(logits.size(0)), max_similar]\n",
    "\n",
    "    # im = torchvision.utils.make_grid(\n",
    "    #     [logits[0].detach().view(1, 28, 28), targets[0].detach().view(1, 28, 28)]\n",
    "    # )\n",
    "    # plt.imshow(im.permute(1, 2, 0), cmap=\"gray\")\n",
    "\n",
    "    return torch.pow(logits - targets, 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, features: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(features, 512), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 400), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(400, 512), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, features), \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.classify = nn.Linear(400, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        logits = self.classify(encoded)\n",
    "        return decoded, encoded, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 28\n",
    "H = 28\n",
    "data = [generate_shapes_image(W, H, 2) for _ in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaec70d98a034b49a6f2093b23352456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 150.36079287711922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dc123c9fd644b09c067cbaed7db0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 111.45987892638692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41c28943454a6ebf8a1e6337354977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 95.73413958147054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c3e071e5146289c1c0e50e9fcc535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 87.51148098996838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f141c92830fa4adabd343ad57d3254cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 78.05429789355344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e43fffab1a45da8b1289893339384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 70.83565665632868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7008eb5ae75a4ce4bac74c47fed1b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 65.66018131382936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc7eca97e0b40a79d793b0e7221efa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 61.455911192442755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdb98333dfb4106ad64630079609ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 58.657496557211324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246de11f41db40d9b12225089618f1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 56.061793266354925\n"
     ]
    }
   ],
   "source": [
    "model = Model(784, len(class_names))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "cortical_delay = 8\n",
    "iterations = 3\n",
    "s_refractory = 4\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch_idx in range(10):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, clusters, targets in tqdm(dataloader):\n",
    "        s_queue = deque([torch.zeros_like(inputs) for _ in range(3)], maxlen=3)\n",
    "        in_refactory = torch.zeros_like(inputs)\n",
    "        context_queue = deque(\n",
    "            [rand_context_like(inputs) for _ in range(cortical_delay)],\n",
    "            maxlen=cortical_delay,\n",
    "        )\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for _ in range(iterations * cortical_delay):\n",
    "            in_refactory -= 1\n",
    "\n",
    "            context_input = context_queue.popleft()\n",
    "            activity = inputs * context_input\n",
    "\n",
    "            # maybe should also add some sort of dropout here\n",
    "            refactory_mask = (in_refactory < 0).float()\n",
    "\n",
    "            activity = activity * refactory_mask\n",
    "            # update refractory period for the neurons that spiked\n",
    "            in_refactory[activity > 0.5] = s_refractory\n",
    "\n",
    "            # accumulate all spikes over the last 3 time steps\n",
    "            s_queue.append(activity)\n",
    "            accumulate_input = torch.stack(tuple(s_queue), dim=0).sum(dim=0)\n",
    "            # accumulate_input = inputs\n",
    "\n",
    "            # feed the accumulated spikes to the denoising autoencoder\n",
    "            # returns the denoised outputs and the latent variables\n",
    "            output, encoding, logits = model(flatten(accumulate_input))\n",
    "\n",
    "            for i in range(1, len(s_queue)):\n",
    "                # randomly \"forget\" half of the spikes\n",
    "                s_queue[i] *= random_mask_like(inputs, p=0.5)\n",
    "\n",
    "            context_queue.append(output.view(inputs.shape))\n",
    "\n",
    "            loss = loss + match_any_loss(logits, targets)\n",
    "            loss = loss + match_cluster_loss(output, clusters.flatten(start_dim=-2)) / 1000\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch_idx} loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2, 2, 1, 2, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.8000)\n",
      "tensor([1, 0, 2, 2, 1, 2, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.8000)\n",
      "tensor([1, 0, 1, 2, 1, 2, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.9000)\n",
      "tensor([0, 0, 2, 2, 1, 2, 2, 2, 0, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.6000)\n",
      "tensor([0, 0, 1, 0, 2, 1, 2, 2, 0, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.7000)\n",
      "tensor([1, 0, 2, 2, 2, 1, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.7000)\n",
      "tensor([1, 0, 2, 0, 1, 2, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.9000)\n",
      "tensor([1, 0, 1, 2, 1, 2, 1, 2, 2, 0])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [2, 1],\n",
      "        [1, 1],\n",
      "        [2, 2],\n",
      "        [0, 2],\n",
      "        [0, 0]])\n",
      "tensor(0.9000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5)\n",
    "\n",
    "test_data = [generate_shapes_image(W, H, 2) for _ in range(10)]\n",
    "inputs = torch.stack([x[0] for x in test_data])\n",
    "targets = torch.stack([x[2] for x in test_data])\n",
    "\n",
    "s_queue = deque([torch.zeros_like(inputs) for _ in range(3)], maxlen=3)\n",
    "in_refactory = torch.zeros_like(inputs)\n",
    "context_queue = deque(\n",
    "    [rand_context_like(inputs) for _ in range(cortical_delay)], maxlen=cortical_delay\n",
    ")\n",
    "\n",
    "for _ in range(cortical_delay):\n",
    "    in_refactory -= 1\n",
    "\n",
    "    context_input = context_queue.popleft()\n",
    "    activity = inputs * context_input\n",
    "\n",
    "    # maybe should also add some sort of dropout here\n",
    "    refactory_mask = (in_refactory < 0).float()\n",
    "\n",
    "    activity = activity * refactory_mask\n",
    "    # update refractory period for the neurons that spiked\n",
    "    in_refactory[activity > 0.5] = s_refractory\n",
    "\n",
    "    # accumulate all spikes over the last 3 time steps\n",
    "    s_queue.append(activity)\n",
    "    accumulate_input = torch.stack(tuple(s_queue), dim=0).sum(dim=0)\n",
    "\n",
    "    # feed the accumulated spikes to the denoising autoencoder\n",
    "    # returns the denoised outputs and the latent variables\n",
    "    output, encoding, logits = model(flatten(accumulate_input))\n",
    "\n",
    "    for i in range(1, len(s_queue)):\n",
    "        # randomly \"forget\" half of the spikes\n",
    "        s_queue[i] *= random_mask_like(inputs, p=0.5)\n",
    "\n",
    "    context_queue.append(output.view(inputs.shape))\n",
    "\n",
    "    loss = loss + match_any_loss(logits, targets)\n",
    "\n",
    "    pred = logits.argmax(dim=1)\n",
    "    print(pred)\n",
    "    print(targets)\n",
    "    # 1 if pred is in targets on the 1st dimension\n",
    "    print((pred.unsqueeze(1) == targets).any(dim=1).sum() / len(pred))\n",
    "\n",
    "i = 3\n",
    "axes[0].imshow(inputs[i].detach().numpy(), cmap=\"gray\")\n",
    "axes[1].imshow(activity[i].detach().numpy(), cmap=\"gray\")\n",
    "axes[2].imshow(accumulate_input[i].detach().numpy(), cmap=\"gray\")\n",
    "axes[3].imshow(output.view(inputs.shape)[i].detach().numpy(), cmap=\"gray\")\n",
    "axes[4].imshow(in_refactory[i].detach().numpy(), cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[3].axis(\"off\")\n",
    "axes[4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
